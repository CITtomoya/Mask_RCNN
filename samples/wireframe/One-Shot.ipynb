{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run on Google CoLab\n",
    "# !git clone https://github.com/Holstrup/Mask_RCNN \n",
    "# import os\n",
    "# os.chdir(\"Mask_RCNN/samples/wireframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - One Shot Version\n",
    "\n",
    "In the document we can run one-shot detection of wireframes using Mask R-CNN as the backbone of that analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "from samples.wireframe.WireframeGenerator import generate_data\n",
    "\n",
    "from samples.wireframe import Wireframe\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "\n",
    "First we need to generate some data, if the data is not already there. Note that, if you already have the data, you can simply skip this block. \n",
    "The function will create an additional 20% of images, that will be used as validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_IMAGES = 100\n",
    "MAX_ICONS_PER_IMAGE = 3\n",
    "generate_data(NUM_TRAINING_IMAGES, MAX_ICONS_PER_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Wireframe.WireframeConfig()\n",
    "WIREFRAME_DIR = os.path.join(ROOT_DIR, \"datasets/wireframe\")\n",
    "\n",
    "# Training dataset\n",
    "dataset_train = Wireframe.WireframeDataset()\n",
    "dataset_train.load_wireframe(WIREFRAME_DIR, \"train\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = Wireframe.WireframeDataset()\n",
    "dataset_val.load_wireframe(WIREFRAME_DIR, \"val\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Shot Inference \n",
    "\n",
    "Now we are ready to start detecting with our model. First we need to load the weights from the pretrained model, and initialize it in inference mode. The model file should be located in the **root folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/alexanderholstrup/git/Mask_RCNN/mrcnn/model.py:773: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Loading weights from  /Users/alexanderholstrup/git/Mask_RCNN/mask_rcnn_20.h5\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE_NAME = \"mask_rcnn_20.h5\"\n",
    "\n",
    "class InferenceConfig(Wireframe.WireframeConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = os.path.join(ROOT_DIR, MODEL_FILE_NAME)\n",
    "# model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we randomly pick an image and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (1024, 1024, 3)       min:    0.00000  max:  255.00000  uint8\n",
      "image_meta               shape: (17,)                 min:    0.00000  max: 1334.00000  float64\n",
      "gt_class_id              shape: (3,)                  min:    0.00000  max:    4.00000  int64\n",
      "gt_bbox                  shape: (3, 4)                min:   10.00000  max:  727.00000  int32\n",
      "gt_mask                  shape: (1024, 1024, 3)       min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions might look similar in naming style, but they do different things. The OneShotAnchor function, is meant for you to tell our model \"*hey! here's a new icon I'd like for you to be able to classify from now on*\". Be aware, that the function only accepts images with **ONE** icon in it. <br/>\n",
    "The OneShotDetect function is meant to detect all icons on a given image, and output their labels, based on the one-shots the models has been given before through the OneShotAnchor function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an anchor\n",
    "\n",
    "Please specify the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"Home\"\n",
    "model.OneShotAnchor(original_image, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect\n",
    "\n",
    "For each icon on the image, it outputs the top 2 guesses, along with the normalized distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (0,1024) into shape (1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ea8f567e8063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneShotDetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mOneShotDetect\u001b[0;34m(self, images, verbose)\u001b[0m\n\u001b[1;32m   2577\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Mask_RCNN/samples/wireframe/knn.py\u001b[0m in \u001b[0;36mknn\u001b[0;34m(embedding)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPredicted\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmatrix_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_known_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdist_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnorm_dist_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_vector\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/Mask_RCNN/samples/wireframe/database_actions.py\u001b[0m in \u001b[0;36mget_known_encodings\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mencodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (0,1024) into shape (1024)"
     ]
    }
   ],
   "source": [
    "model.OneShotDetect(original_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations & Metrics\n",
    "\n",
    "To be made"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
