{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:39.918555Z",
     "iopub.status.busy": "2024-05-25T07:35:39.918269Z",
     "iopub.status.idle": "2024-05-25T07:35:39.980393Z",
     "shell.execute_reply": "2024-05-25T07:35:39.979668Z",
     "shell.execute_reply.started": "2024-05-25T07:35:39.918527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "#sys.path：这是一个由字符串组成的列表，每个字符串表示一个目录的绝对路径。\n",
    "# 当你尝试导入一个模块（例如 import some_module）时，Python 解释器会按照 sys.path 列表中的顺序来查找这个模块。\n",
    "\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "#用于将图像嵌入显示进web页面当中\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:44.757391Z",
     "iopub.status.busy": "2024-05-25T07:35:44.757083Z",
     "iopub.status.idle": "2024-05-25T07:35:44.770940Z",
     "shell.execute_reply": "2024-05-25T07:35:44.770255Z",
     "shell.execute_reply.started": "2024-05-25T07:35:44.757364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     4\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:47.276593Z",
     "iopub.status.busy": "2024-05-25T07:35:47.276307Z",
     "iopub.status.idle": "2024-05-25T07:35:47.280953Z",
     "shell.execute_reply": "2024-05-25T07:35:47.280260Z",
     "shell.execute_reply.started": "2024-05-25T07:35:47.276566Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:48.801266Z",
     "iopub.status.busy": "2024-05-25T07:35:48.800974Z",
     "iopub.status.idle": "2024-05-25T07:35:48.825893Z",
     "shell.execute_reply": "2024-05-25T07:35:48.825197Z",
     "shell.execute_reply.started": "2024-05-25T07:35:48.801238Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:51.501036Z",
     "iopub.status.busy": "2024-05-25T07:35:51.500730Z",
     "iopub.status.idle": "2024-05-25T07:35:51.756009Z",
     "shell.execute_reply": "2024-05-25T07:35:51.755296Z",
     "shell.execute_reply.started": "2024-05-25T07:35:51.501007Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:53.194021Z",
     "iopub.status.busy": "2024-05-25T07:35:53.193719Z",
     "iopub.status.idle": "2024-05-25T07:35:54.041049Z",
     "shell.execute_reply": "2024-05-25T07:35:54.040357Z",
     "shell.execute_reply.started": "2024-05-25T07:35:53.193991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAW/klEQVR4nO3de5ScdX3H8c937pe9zN53k+wm2WRzIXeSkARyJ1xCREoMgtzKRZFjQUEtitKLoKKoFFGr1lO8tPW0tlgOVq1UEQUEFOS0xctRtKg9asFjbT0lye7M/PrHzMK62c0+uzszv2d23i/Onsw888zz+07y7OH5zO/3e37mnBMAAAAABBHxXQAAAACA+kGAAAAAABAYAQIAAABAYAQIAAAAAIERIAAAAAAERoAAAAAAEJj3AGFmi8zsy+O2PT2D43zRzDaUH59lZr82Mys/v83MLglwjFvM7Cdj6zGzDWb2sJl93czuN7PB8vY2M7vPzL5Wfn3tcY7bbGaPmNlvzOziMdtvMLPHyu//wJh695vZt8zsQTP7GzOLTffvA/6YWc7MLp3ktTvMrKtC7RzzuwMAAFBt3gNEBT0k6ZTy41MkfVvSqjHPHwxwjD+XtGfctl9IOtM5t1PSeyW9rbz9IkkPO+d2SXpr+WcyhyWdK+mOcdv/0Tm3xTl3iqQeSXvL22+RdMg5t0PSiKTTAtSO8MhJOiZAmFnUOXedc+652pcEAABQGXUTIMzsw2Z2qZlFzOxLZrZl3C4PSdpefrxO0oclbTezpKRe59wzU7XhnPuFpOK4bb90zv22/HRYUr78+HuSWsqP2yU9ayX3mtluM8uUex0WO+fyzrlfTtDeD8c8HXvs70jKlXskWiVxwVlfXi9po5k9UO5J+oSZ3Svp5eVtC8ys08y+Un7+sJktk6Tyvh80s8+b2aNm1l3e/noze7zcI/UtM1s0tkEz6y+/5/7ynxXp5QAAABgvLENjNprZA1Psc72k+1XqTfiKc+6xca8/JukuM4tLcpK+Lul9kp6S9E1JMrNtkm6d4Ng3O+fuP17jZpaV9A5Jl5c3PSHpZjN7SqVvnLc755yZXSnpC5KelnSHc+4/pvhcMrPdkvrKNUvSpyT9s6T/lfSvzrnHpzoGQuV2SSc45/aZ2Z9K6nPOvVSSzOzV5X3+R9J+59ywme2X9GZJV5Rfe9o5d42ZvUWl0PEZSZdIOklSWtKPJ2jzPZJucc49ambnSHqTpDdW6fMBAIAGFpYA8YRzbt/ok4nmQDjnjpjZxyXdptLF9kSvPyvpoKQnnXPPmVmvSr0SD5X3eUTS7ukWVw4lfyfpVufcd8ubb5B0t3Pu9nIw+ZCkA+V275N0rnPuFQGOvValUHO2c86VN39U0knOuZ+Z2UfM7Dzn3N9Pt26Exjcm2JaT9KHyOZqQ9Nsxrz1R/vOnkpZIWizpKefciKQRM/v+BMdbI+ld5Wk0MZUCLDBjZnaNpEMqBdpX+q4HjYnzEL5xDk6snoYw9Um6UtLbJb1zkt0eUunC/uHy859LOk/l+Q9mtq08ZGT8z95Jjiczi0j6a0n3OOfuGfuSpF+VHz+r0jAmmdlqSSdLutfMXjvFZ1oq6S5JFzjnfjXmpYKk/y4/fm702Kgbw/rdcF6YYJ+LVQq6OyXdrNL5NMqNeWySnpG0ysxiZtYsafkEx/uOpOudc7udc9slXTWL+gE55z5YPp/4Hya84TyEb5yDEwtLD8RxlS/iPy7puvIQjb81swPOuc+P2/VBlcafP1p+/rCk31NpGNOUPRDllHmBpJXlu9u8WtIGSQck9ZTvoPTvzrlrJX1A0l+Z2RUqDSt5k5mlJf2FSheHP5V0n5k96Jx70sw+p9Kk7ufNbLtz7mqVJlXnJH2y/M3xe8qf6SZJ95vZEUm/kfTu6f+twaNfSjpsZndL6tbEvQH3Sfq0me2Q9N0JXn+Bc+6/zOzTKg3T+4Gk/1QppCTG7PYGlXo0msrP71Ip+AIAAFSUvThqBkBYmVncOTdiZi2SnpS0zDk3Uc8GAABAVdVFDwQAvdnMTlXprlx/RHgAAAC+0AMBAAAAILC6mUQNAAAAwD8CBAAAAIDAjjsH4gF3F+ObGshuu8Km3qv20huu4TxsIIef/GDozkPOwcYSxnNQ4jxsNJyHCIPJzkN6IAAAAAAERoAAAAAAEBgBAgAAAEBgBAgAAAAAgREgAAAAAARGgAAAAAAQGAECAAAAQGAECAAAAACB1W2AiDhTphj3XQYaXSIt9SzxXQUAAEDN1GWAiDhTT6FZy/LdaiomfJeDRpVIq/3Ek3XGoR3SvOW+qwEAAKiJugwQSRfTYKFDWZfQQL7NdzloVK09Ovf0FVrQkdVJ+070XQ0AAEBN1F2AiDhTezHzwvOEYmoppmrWftTF1VKYV7P2EFKJtLpXvNjrkGtKSIvW1a791h4tOXBO7doDAAAoq7sAEVdUiwrtLzzPuoT6Ci01aTviYuoeWamB4c1qzS+oSZsIqUyrzt6z9IWn/Z1NWrdlWW3abu7Qq68/pH+6bodWnnuwNm0CAACU1VWAiDhTb6H5mO1pF1dbIV319mMuqb78aiVcVj35FVVvDyEVT6p/86ZjNvd0ZBRbtrn67XcN6F0HVqo3l9KHL2LoFAAAqK26ChBRRbSgkDtme9Yl1FnMVrXtiIupe0xoSLis2vOLq9omQiqZ1enbFh6zub+zSctWD1S37eYO3Xzt7hee9uZS2nrZhdVtEwAAYIy6CRARZxrI5yZ9vckl1VHITPr6rNtXTN35F8e8lwLEsReRmOPiSQ3t2Tnpy/19zUqesLV67Td16NrtL942tqc1pZvOqNHQKQAAANVJgDBnWpLvUG9x8rkOGZdQa7E6w5giLqYFwxuO2Z5yOXWOLJ3gHZiTYgmdcPZLtHPD/El3WdCRVf9gb3Xaz+b0sVuOnfMw1NWkU6++tDptAgAAjFMXASIiqbt47NyH8XIurc5C5YcymSJqLxw7XCnhMmouVuliEeETiWrb6qn/vZcvblN6zcmVbz/VrEPrjp28392a0uVb+yvfHgAAwARCHyDMScvy3YH2Tbu4BvJt6qhgiDAX1aKjk18MZoud6hphEbE5LxLVppcHu21qb1tGe3cvV2r1tsq1n27W5z5w5aQvbxlo1znXTf46AABApYQ+QEi/u+7DVNKKK+3ilWnZRbT06G61Fidf9yHhMsoUWcxuzjPTusGOwLv3tmXU2tFambbjSX35kzdo+1DnpLt0Nid11qrJXwcAAKiUcAcIJ60Zmf4Qod5Cc8WGMjUXe6bcp6UwT90jKyvSHkLITLuueMW037btxPmVGcoUiWrj4qlD6mlDPTr0hlfNvj0AAIDjCG+AcNL6kXlqdtNfZTqpmBIuOsv2TSuOnBFo17hS6h05Qd0MZZqT9l19qZb2TX+xwo7mlDLNs7wzWDSmb/3DHwfatS2b0HtfegJDmQAAQFWFN0BIyrrkjN+7oJCbdS9E2gUfmhRTUnFX/cXsUHsLu5pm/N69Jy+e3VwIi2hpb/D2WzNxLe2p7pooAACgsYUzQDhp4/Ds7ioTV1TRmX48J60+8tJpv60zv1RdI9yTfy6Z7bf5LZmEkqkZBmEzfe+L75z2267fPqgD114+szYBAACmEM4AISmpWQ5BkrQo3z79xeWctObwQSXc9L/FjSqhqCozgRvh0NE8816wUQf2LZ/R4nI/uv996s1NfwhfNhVTZ8v03wcAABBEKAPEluEBmWzWx4kpoqF8l3KF6Q0timnmF429I6vVOTI04/cjPC644SqZzf48TCdiOnTOekWHNk3rfe1NiRm3+e4DK3TmH1w24/cDAABMJpQBYsZDjyY5VmQaYWTd4fNmFV4iiipSgd4T+JdJxip2rFQ8qkg0+Hn984ffP6v2kvGo0hWsHwAAYFToAsTWowsrfszl+W61FoMN6Yho9hdd80fWqyO/ZNbHgT8X3Xh1xY954cs2SoMbAu2bis/+V/Nj56/TzldePOvjAAAAjBWqALH16EJFZBUZvjRW0B6I9c+fX5H2TBENDJ+ktnzlwxCq75K3XK1kLFKR4UtjxaLBjvnsI3dWpO1oxHTPVVu06eILZn0sAACAUeEJEE7l6FDZi7ZRq0Z61Vw8ztyGCrdfOo5JriKHQw2ZrOLhYdTlF26VBlYfd59IpHJtm5mq9FEAAECDCk2A2Dq8cFpzFabrhWAw0QW9k9YfPl9W4b+ORcPb1FqYX9FjorouuvFqxWPV+7U4bjAx07OP3KloBQOEJN332u1afs65FT0mAABoXOEIEDX6ln7tyDxl3bF3tll3+OVVmfhML0SdqdFX9VdetkvqXXrM9l88dEfVwks0GqnZ5wMAAHOb9wBhrnTb1kreeem47Y27oDdX3XaXDO9Uc7G3qm2gAqIxXfCHr1IqXqM7aEXHrReSnOZ6JdP08I171L/vQFXbAAAAjcF7gNg03K9YDW97um60F6IcItYcPqhoBe68dDwRF5Uc3/6G2aHrLlc2VbtFAK+8cu/v9EL85F/eoVSiur8HmUxcinJrVwAAMDteA0TURaQqznuYzPqR+UoqpugEw5mqYcnwLmWLHTVpCzOQavIyuufKq86QWnuktnmK1KCAR286VZ1b91S9HQAAMLd5DRDrh+cp4WnRtbiL6ITDBxRTbUJEzCXphQipc64+Xy2Z2pwHx8jm9KPPvlFNqdr0DHR0NkkxT58VAADMCd4CRMJFq3bL1iDWDC9WvMpDl8ZaMrxLLcU+GSEiXFq6Knrb1Ol66/X7lany0KWxHr3pVA3tP0uKH+eWxgAAAMfhLUCsHulTsoYX8GMVXUpHi2sk1W7MuyQtPbpbCddU0zZxfPsvfYnam/xcTPfnEnrNtkVVn/sw3jf/eJ9ig2tr2iYAAJg7vASIdDHu9Zv44eIKqUZDl8ZLF3NVv/MTAupapEQV13yYymUbB5RJ+gnRy1cPSIm0l7YBAEB9q/nVU6aY0Mp8j1Leeh+y8jn1Y3B4h+Iu5a19lPUs0YEL9qizxc+/xfKulGIeh0499OY90rxl3toHAAD1q+ZX0kvyHUq72g4dGmu4uEROfsd/NxV76IXwbMfZ29TbVt21F47n4Kp5ytZo4vRktu1ZRS8EAACYtppexbYUk4p5/Pa/4FokT3d9GmvR8DZFPQ2hgqSFa9VUwzUfxtu4IOt16NSoL7zmZKltnu8yAABAnanpVUx/vk2ZGq29MF7BtWqkOCincAwfassP0AvhyZbdqzSv3U/vw0n9TTpreZ+a0/4CzFj7z9tJLwQAAJiWml3B5gppb2s+SFK+OE9O4blQ6h/ZpK78MtaGqLHo0Ca1ebrrkiSdMtCp1kw4woMkffr3N+rgtRexNgQAAAisZgGit9jssfeh3fu8h4ksGDlREb9r+TWcdZsHNb8j66XtXYtba7Zg3HT85QXr6YUAAACB1eTqtaOQ8RYeJClf7ApV78NYvSOr6YWokeSqrepp9xMeJGltby5UvQ9jXffWS6Vo+MINAAAIn6oHiI5CRgOFNm93XsoXO+Xk7247U+nNr/K6InejSK7aqj27V3ib+7BvSU5t2XCGB0n6k9OXS9Hw1gcAAMKj6gEiV0x77X0oKheaidOT6R/eLDnfVcxtA4O93oYuSdJQR7NaQjJxejJ3vv8aKeL/LmUAACDcqhogugpZtRb9DR3KF7tVdM3e2g+qs7BEoheiajJrT9Hyxe3e2t8/1K6e1nCHWEm6ZNNCyTgPAQDA8VU1QGRdUmn5GrrUpbybH/reh1GDwzvohaiSrr529eT8BNn9Q+3asqgjlJOnJ3L3J97iuwQAABByVQsQ3YUmdRT8DRlxytRNeJCkXGGB7xLmpOz67Vq/sttb+z3ZVN2EB0nau6KbXggAAHBcVQkQXYWsBvJtSsnPhVO+2K286/TS9mwsO3oavRAVlFl7ik7fu0KdLX6C5FlDHRrs9heiZ+rBu9/uuwQAABBiVQkQCRdT0lN4kFRe86H+FsbKFju14siZvsuYMzLNGbV7XDSuPZVQJlk/vQ+jVve36rF7b/VdBgAACKmKB4jOQlbzC62VPmxgpd6HHm/tz4bJlHZtvsuYE9JrTtaebYu8tb9/qF0r5od/Av9khnqbfJcAAABCquIBIqaI4vJ3K0inmORp4nalrDp8tu8S6l4ilVBLxl8vVDYeUypev7dENTM99aX3+C4DAACEUEUDREchq4V5f7fLLN15qb4nI5tMCVd/4+bDJLlqqw6cutxb+2csbdfGhfXfk9TT6m/4FwAACK+KBoiITLHqr003oXyxQyNuseSx96NyTGueP9d3EXUrFospnfAz92DvYE57lnUpHvPze1BJsWhET3/1dt9lAACAkKnYVU57IaOleZ93PjLNjfBQ6oWI1uEk8DBIrNyilx1Y4639qJli0foPD6Oa6+gWtAAAoDYqdnXw68jzejTxTKUONy25Qr8WF5Z6abtaTBGtff6g/i3zWd+l1JXh739Tn3r3E17aXnfoXN12YIWXtqslEYvoma/9mRbtut53KQAAICQq91WpSc7Xz2gBc0ipFyKpNc8f9F1KfXFOKuS9/BQKTpHI3DoPJak1E9ePGMoEAADK6n6sRXOhT4uHt/suoyqs/B+Ly4XfojPP1oNv2u27jKqJzsFgBAAAZqa+A4Qr9TvYHOt9GCumpNYcZkJ12JnN3XNQKvVC/PD+9/kuAwAAhED9BggnNRW7tfToHt+V1Iab2xeo9axvz1n69s2n+y6jNqJMqgYAoNHVbYDIFNu17Og+32XURFxprTrC4nJh1Lp5t75721m+y6iJzuakvvPFW32XAQAAPKvPAOGkSOVuIFU3Im5u3KZ2zohElcqkfFdRU2YmZXO+ywAAAB7VZYBIudaG6X0YlXRNWn7kDN9lYIzkys36/ntf4ruMmurLpfTtz9zouwwAAOBR/QUIZ4q5pO8qvDBFFG3Qzx460Zjau9t9V+FFNGJS54DvMgAAgCf1FSCcqanY1XC9D6NSrkXLjpyqeDHtu5TGFo2pZ/u+hpn7MN5AZ0aPf+I10rzlvksBAAAe1FWAiCnZsOFhVNrltHj4FN9lNLbuwYYbujTekp4mPXDnJb7LAAAAHtRPgHCmTLHNdxWhEHExJYvNvstoTNGYFq9f4buKUEgnooot2+y7DAAAUGN1EyCiijfOmg9TyLh29Q9v9F1GY8r1Nc6aD1NY1tese952wHcZAACgxuojQDhTc6HXdxWhEnMppemRqa1oTOtP2+q7ilDpyCTVtGGH7zIAAEAN1UWAiCiqweHtvssIlYxrV+/IKt9lNJZMq776hl2+qwiVFfOa9dHX7fRdBgAAqKHwBwhnas8v8l1FKCVcVtlCp+8yGkMkqr0XMlxnIotzWXVtZ1gXAACNoi6Wc067nJ6L/cB3GaGUdM36P/3KdxlzXySq007o1Ee+8WPflYTS0qFOPfeQ7yoAAEAthD9AmNPPEo/7rgKNLj+sG193u+8qAAAAvAv/ECYAAAAAoUGAAAAAABAYAQIAAABAYAQIAAAAAIERIAAAAAAERoAAAAAAEBgBAgAAAEBgBAgAAAAAgREgAAAAAARGgAAAAAAQGAECAAAAQGAECAAAAACBESAAAAAABEaAAAAAABAYAQIAAABAYAQIAAAAAIERIAAAAAAERoAAAAAAEBgBAgAAAEBgBAgAAAAAgREgAAAAAARGgAAAAAAQGAECAAAAQGAECAAAAACBESAAAAAABEaAAAAAABAYAQIAAABAYAQIAAAAAIERIAAAAAAERoAAAAAAEBgBAgAAAEBgBAgAAAAAgREgAAAAAARGgAAAAAAQGAECAAAAQGAECAAAAACBESAAAAAABEaAAAAAABAYAQIAAABAYAQIAAAAAIERIAAAAAAERoAAAAAAEBgBAgAAAEBgBAgAAAAAgREgAAAAAARGgAAAAAAQGAECAAAAQGDmnPNdAwAAAIA6QQ8EAAAAgMAIEAAAAAACI0AAAAAACIwAAQAAACAwAgQAAACAwAgQAAAAAAL7f3HNlMXzlNEEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:35:55.848017Z",
     "iopub.status.busy": "2024-05-25T07:35:55.847691Z",
     "iopub.status.idle": "2024-05-25T07:36:00.888429Z",
     "shell.execute_reply": "2024-05-25T07:36:00.887712Z",
     "shell.execute_reply.started": "2024-05-25T07:35:55.847986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /mnt/workspace/maskRCNN_learn_env/Mask_RCNN/mrcnn/model.py:605: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:36:04.524164Z",
     "iopub.status.busy": "2024-05-25T07:36:04.523862Z",
     "iopub.status.idle": "2024-05-25T07:36:09.387460Z",
     "shell.execute_reply": "2024-05-25T07:36:09.386690Z",
     "shell.execute_reply.started": "2024-05-25T07:36:04.524130Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:36:11.228010Z",
     "iopub.status.busy": "2024-05-25T07:36:11.227696Z",
     "iopub.status.idle": "2024-05-25T08:06:12.927708Z",
     "shell.execute_reply": "2024-05-25T08:06:12.925722Z",
     "shell.execute_reply.started": "2024-05-25T07:36:11.227981Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /mnt/workspace/maskRCNN_learn_env/Mask_RCNN/logs/shapes20240531T0731/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/pai/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/pai/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.6/site-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pai/lib/python3.6/site-packages/keras/callbacks.py:1128: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.6/site-packages/keras/utils/data_utils.py:709: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m/mnt/workspace/maskRCNN_learn_env/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[1;32m   2382\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2385\u001b[0m         )\n\u001b[1;32m   2386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pai/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T07:06:07.589007Z",
     "iopub.status.busy": "2024-05-25T07:06:07.588537Z",
     "iopub.status.idle": "2024-05-25T07:06:07.613114Z",
     "shell.execute_reply": "2024-05-25T07:06:07.612291Z",
     "shell.execute_reply.started": "2024-05-25T07:06:07.588977Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /mnt/workspace/maskRCNN_learn_env/Mask_RCNN/logs/shapes20240525T0843/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/pai/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/pai/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/pai/lib/python3.6/site-packages/keras/engine/training_generator.py:49: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the `keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.6/site-packages/keras/utils/data_utils.py:709: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-25T08:06:12.929313Z",
     "iopub.status.idle": "2024-05-25T08:06:12.929616Z",
     "shell.execute_reply": "2024-05-25T08:06:12.929472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-25T08:06:12.930644Z",
     "iopub.status.idle": "2024-05-25T08:06:12.930942Z",
     "shell.execute_reply": "2024-05-25T08:06:12.930785Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-25T08:06:12.931833Z",
     "iopub.status.idle": "2024-05-25T08:06:12.932101Z",
     "shell.execute_reply": "2024-05-25T08:06:12.931966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-25T08:06:12.933038Z",
     "iopub.status.idle": "2024-05-25T08:06:12.933316Z",
     "shell.execute_reply": "2024-05-25T08:06:12.933170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-25T08:06:12.934169Z",
     "iopub.status.idle": "2024-05-25T08:06:12.934447Z",
     "shell.execute_reply": "2024-05-25T08:06:12.934313Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3f128cad0f25ad952b91777a639b4fa2222dfd0378c44e8bdbd611468a92202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
