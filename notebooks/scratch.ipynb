{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than saving landsat tifs to disk then transferring to cloud storage, can we modify lsru to save directly to azure? This would save us from having to montior when an instance fills up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test azure functions in ecohydro lsru to see if we can save direct to azure, no saving to instance store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lsru import Usgs\n",
    "from lsru import Espa\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import time\n",
    "import yaml\n",
    "from azure.storage.file.fileservice import FileService\n",
    "\n",
    "config_path = \"/home/ryan/work/azure_configs.yaml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "\n",
    "# Instantiate Usgs class and login. requires setting config with credentials\n",
    "usgs = Usgs(conf=configs[\"download\"][\"lsru_config\"])\n",
    "usgs.login()\n",
    "espa = Espa(conf=configs[\"download\"][\"lsru_config\"])\n",
    "\n",
    "file_service = FileService(\n",
    "    configs[\"storage\"][\"storage_name\"], configs[\"storage\"][\"storage_key\"]\n",
    ")\n",
    "\n",
    "DATA_DIR = configs[\"storage\"][\"vm_temp_path\"]\n",
    "WBD_PATH = configs[\"storage\"][\"wbd_gdb_path\"]\n",
    "\n",
    "REGION_DIR = os.path.join(\n",
    "    configs[\"storage\"][\"region_dir\"], configs[\"storage\"][\"region_name\"]\n",
    ")\n",
    "\n",
    "LANDSAT_DWNLD_DIR = os.path.join(REGION_DIR, \"landsat_downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LT050320312005101501T1-SC20190411045817.tar.gz\n",
      "LT050320312005101501T1-SC20190411045817.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005103101T1-SC20190411010427.tar.gz\n",
      "LT050320312005103101T1-SC20190411010427.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005092901T1-SC20190411010314.tar.gz\n",
      "LT050320312005092901T1-SC20190411010314.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005071101T1-SC20190411005910.tar.gz\n",
      "LT050320312005071101T1-SC20190411005910.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005040601T1-SC20190411005728.tar.gz\n",
      "LT050320312005040601T1-SC20190411005728.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005021701T1-SC20190411005603.tar.gz\n",
      "LT050320312005021701T1-SC20190411005603.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005082801T1-SC20190411005223.tar.gz\n",
      "LT050320312005082801T1-SC20190411005223.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005030501T1-SC20190411004807.tar.gz\n",
      "LT050320312005030501T1-SC20190411004807.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n",
      "Downloading LT050320312005011601T1-SC20190411001516.tar.gz\n",
      "LT050320312005011601T1-SC20190411001516.tar.gz skipped. Reason: 'NoneType' object has no attribute 'read'\n"
     ]
    }
   ],
   "source": [
    "espa.orders[0].download_all_complete_azure(configs['storage']['container'],configs[\"storage\"][\"storage_name\"], configs[\"storage\"][\"storage_key\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to figure out how to use blob storage, try saving and reading landsat tif files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Azure-Samples/storage-blob-python-getting-started\n",
    "\n",
    "newer version\n",
    "\n",
    "https://github.com/Azure-Samples/storage-blobs-python-quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from azure.storage.blob import BlockBlobService\n",
    "config_path = \"/home/ryan/work/azure_configs.yaml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    configs = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Azure Storage Blob Sample - Demonstrate how to use the Blob Storage service. \n",
    "# Blob storage stores unstructured data such as text, binary data, documents or media files. \n",
    "# Blobs can be accessed from anywhere in the world via HTTP or HTTPS. \n",
    "#\n",
    " \n",
    "# Documentation References: \n",
    "#  - What is a Storage Account - http://azure.microsoft.com/en-us/documentation/articles/storage-whatis-account/ \n",
    "#  - Getting Started with Blobs - https://azure.microsoft.com/en-us/documentation/articles/storage-python-how-to-use-blob-storage/\n",
    "#  - Blob Service Concepts - http://msdn.microsoft.com/en-us/library/dd179376.aspx \n",
    "#  - Blob Service REST API - http://msdn.microsoft.com/en-us/library/dd135733.aspx \n",
    "#  - Blob Service Python API - http://azure.github.io/azure-storage-python/ref/azure.storage.blob.html\n",
    "#  - Storage Emulator - http://azure.microsoft.com/en-us/documentation/articles/storage-use-emulator/ \n",
    "#\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_blob_service = BlockBlobService(account_name=configs['storage']['storage_name'], account_key=configs['storage']['storage_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create_blob_from_bytes in module azure.storage.blob.blockblobservice:\n",
      "\n",
      "create_blob_from_bytes(container_name, blob_name, blob, index=0, count=None, content_settings=None, metadata=None, validate_content=False, progress_callback=None, max_connections=2, lease_id=None, if_modified_since=None, if_unmodified_since=None, if_match=None, if_none_match=None, timeout=None) method of azure.storage.blob.blockblobservice.BlockBlobService instance\n",
      "    Creates a new blob from an array of bytes, or updates the content\n",
      "    of an existing blob, with automatic chunking and progress\n",
      "    notifications.\n",
      "    \n",
      "    :param str container_name:\n",
      "        Name of existing container.\n",
      "    :param str blob_name:\n",
      "        Name of blob to create or update.\n",
      "    :param bytes blob:\n",
      "        Content of blob as an array of bytes.\n",
      "    :param int index:\n",
      "        Start index in the array of bytes.\n",
      "    :param int count:\n",
      "        Number of bytes to upload. Set to None or negative value to upload\n",
      "        all bytes starting from index.\n",
      "    :param ~azure.storage.blob.models.ContentSettings content_settings:\n",
      "        ContentSettings object used to set blob properties.\n",
      "    :param metadata:\n",
      "        Name-value pairs associated with the blob as metadata.\n",
      "    :type metadata: dict(str, str)\n",
      "    :param bool validate_content:\n",
      "        If true, calculates an MD5 hash for each chunk of the blob. The storage\n",
      "        service checks the hash of the content that has arrived with the hash\n",
      "        that was sent. This is primarily valuable for detecting bitflips on\n",
      "        the wire if using http instead of https as https (the default) will\n",
      "        already validate. Note that this MD5 hash is not stored with the\n",
      "        blob.\n",
      "    :param progress_callback:\n",
      "        Callback for progress with signature function(current, total) where\n",
      "        current is the number of bytes transfered so far, and total is the\n",
      "        size of the blob, or None if the total size is unknown.\n",
      "    :type progress_callback: func(current, total)\n",
      "    :param int max_connections:\n",
      "        Maximum number of parallel connections to use when the blob size exceeds\n",
      "        64MB.\n",
      "    :param str lease_id:\n",
      "        Required if the blob has an active lease.\n",
      "    :param datetime if_modified_since:\n",
      "        A DateTime value. Azure expects the date value passed in to be UTC.\n",
      "        If timezone is included, any non-UTC datetimes will be converted to UTC.\n",
      "        If a date is passed in without timezone info, it is assumed to be UTC.\n",
      "        Specify this header to perform the operation only\n",
      "        if the resource has been modified since the specified time.\n",
      "    :param datetime if_unmodified_since:\n",
      "        A DateTime value. Azure expects the date value passed in to be UTC.\n",
      "        If timezone is included, any non-UTC datetimes will be converted to UTC.\n",
      "        If a date is passed in without timezone info, it is assumed to be UTC.\n",
      "        Specify this header to perform the operation only if\n",
      "        the resource has not been modified since the specified date/time.\n",
      "    :param str if_match:\n",
      "        An ETag value, or the wildcard character (*). Specify this header to perform\n",
      "        the operation only if the resource's ETag matches the value specified.\n",
      "    :param str if_none_match:\n",
      "        An ETag value, or the wildcard character (*). Specify this header\n",
      "        to perform the operation only if the resource's ETag does not match\n",
      "        the value specified. Specify the wildcard character (*) to perform\n",
      "        the operation only if the resource does not exist, and fail the\n",
      "        operation if it does exist.\n",
      "    :param int timeout:\n",
      "        The timeout parameter is expressed in seconds. This method may make\n",
      "        multiple calls to the Azure service and the timeout will apply to\n",
      "        each call individually.\n",
      "    :return: ETag and last modified properties for the Block Blob\n",
      "    :rtype: :class:`~azure.storage.blob.models.ResourceProperties`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(block_blob_service.create_blob_from_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sample():\n",
    "    try:\n",
    "        \n",
    "        # Create a container called 'quickstartblobs'.\n",
    "        container_name =configs['storage']['container']\n",
    "        block_blob_service.create_container(container_name)\n",
    "\n",
    "        # Set the permission so the blobs are public.\n",
    "#         block_blob_service.set_container_acl(container_name, public_access=PublicAccess.Container)\n",
    "\n",
    "        # Create a file in Documents to test the upload and download.\n",
    "        local_path=os.path.expanduser(\"~/work/tmp/LT050320312005011601T1-SC20190411001516\")\n",
    "        local_file_name =\"LT05_L1TP_032031_20050116_20160913_01_T1_sr_band1.tif\"\n",
    "        full_path_to_file =os.path.join(local_path, local_file_name)\n",
    "\n",
    "\n",
    "        print(\"Temp file = \" + full_path_to_file)\n",
    "        print(\"\\nUploading to Blob storage as blob\" + local_file_name)\n",
    "\n",
    "        # Upload the created file, use local_file_name for the blob name\n",
    "        block_blob_service.create_blob_from_path(container_name, local_file_name, full_path_to_file)\n",
    "\n",
    "        # List the blobs in the container\n",
    "        print(\"\\nList blobs in the container\")\n",
    "        generator = block_blob_service.list_blobs(container_name)\n",
    "        for blob in generator:\n",
    "            print(\"\\t Blob name: \" + blob.name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp file = /home/ryan/work/tmp/LT050320312005011601T1-SC20190411001516/LT05_L1TP_032031_20050116_20160913_01_T1_sr_band1.tif\n",
      "\n",
      "Uploading to Blob storage as blobLT05_L1TP_032031_20050116_20160913_01_T1_sr_band1.tif\n",
      "\n",
      "List blobs in the container\n",
      "\t Blob name: LT05_L1TP_032031_20050116_20160913_01_T1_sr_band1.tif\n"
     ]
    }
   ],
   "source": [
    "run_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Clean up resources. This includes the container and the temp files\n",
    "        block_blob_service.delete_container(container_name)\n",
    "        os.remove(full_path_to_file)\n",
    "        os.remove(full_path_to_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
