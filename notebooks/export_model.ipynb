{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN - Inspect Trained Model\n",
    "\n",
    "Code and visualizations to test, debug, and evaluate the Mask R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based on the work of Waleed Abdulla (Matterport)\n",
    "Modified by github.com/GustavZ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "# Model  Directory \n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MS COCO Dataset\n",
    "import coco\n",
    "config = coco.CocoConfig()\n",
    "COCO_DIR = os.path.join(ROOT_DIR,\"data/coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       mobilenetv1\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'rpn_class_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           512_cocoperson\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "RES_FACTOR                     2\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_BN                       True\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_MULTIPROCESSING            True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "#DEVICE = \"/cpu:0\"\n",
    "DEVICE = \"/gpu:0\"\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\"\n",
    "#TEST_MODE = \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path to trained h5 weights file\n",
    "MODEL_NAME = 'mask_rcnn_512_cocoperson_0396' # TODO: enter value here\n",
    "H5_MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME+\".h5\") # TODO: enter value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loading weights ', '/home/gustav/workspace/Mobile_Mask_RCNN/logs/mask_rcnn_512_cocoperson_0396.h5')\n"
     ]
    }
   ],
   "source": [
    "import mmrcnn.model as modellib\n",
    "\n",
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=TEST_MODE, model_dir=MODEL_DIR,config=config)\n",
    "\n",
    "# Set path to model weights\n",
    "weights_path = H5_MODEL_PATH\n",
    "#weights_path = model.find_last()[1]\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the pb file we want to output\n",
    "MODEL_NAME = 'mask_rcnn_512_cocoperson_0396' # TODO: enter value here\n",
    "\n",
    "# Chose whether to quantize the graph\n",
    "QUANTIZE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 143 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 143 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 143 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from keras import backend as K\n",
    "\n",
    "# Get keras model and save\n",
    "model_keras= model.keras_model\n",
    "# All new operations will be in test mode from now on.\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Create output layer with customized names\n",
    "num_output = 7\n",
    "pred_node_names = [\"detections\", \"mrcnn_class\", \"mrcnn_bbox\", \"mrcnn_mask\", \"rois\", \"rpn_class\", \"rpn_bbox\"]\n",
    "pred_node_names = [\"output_\" + name for name in pred_node_names]\n",
    "pred = [tf.identity(model_keras.outputs[i], name = pred_node_names[i])for i in range(num_output)]\n",
    "\n",
    "# Get the object detection graph\n",
    "sess = K.get_session()\n",
    "if QUANTIZE:\n",
    "    # Transformations\n",
    "    transforms = [\"quantize_weights\", \"quantize_nodes\"]\n",
    "    transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [], pred_node_names, transforms)\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, transformed_graph_def, pred_node_names)\n",
    "    PB_MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME+'.pb') \n",
    "else:\n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)\n",
    "    PB_MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME+'_quantized'+'.pb') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2377 ops in the frozen graph.\n",
      "('saved the freezed graph (ready for inference) at: ', '/home/gustav/workspace/Mobile_Mask_RCNN/logs/mask_rcnn_512_cocoperson_0396.pb')\n"
     ]
    }
   ],
   "source": [
    "# Write Output pb File\n",
    "graph_io.write_graph(constant_graph, \"/\", PB_MODEL_PATH, as_text=False)\n",
    "\n",
    "# Output Info\n",
    "print('{} ops in the frozen graph.'.format(len(constant_graph.node)))\n",
    "print('saved the freezed graph (ready for inference) at: ', PB_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model\n",
    "Now, we can load the model from the pb file and then use it to infere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mold_inputs(images):\n",
    "        \"\"\"Takes a list of images and modifies them to the format expected\n",
    "        as an input to the neural network.\n",
    "        images: List of image matricies [height,width,depth]. Images can have\n",
    "            different sizes.\n",
    "\n",
    "        Returns 3 Numpy matricies:\n",
    "        molded_images: [N, h, w, 3]. Images resized and normalized.\n",
    "        image_metas: [N, length of meta data]. Details about each image.\n",
    "        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n",
    "            original image (padding excluded).\n",
    "        \"\"\"\n",
    "        molded_images = []\n",
    "        image_metas = []\n",
    "        windows = []\n",
    "        for image in images:\n",
    "            # Resize image to fit the model expected size\n",
    "            # TODO: move resizing to mold_image()\n",
    "            molded_image, window, scale, padding = utils.resize_image(\n",
    "                image,\n",
    "                max_dim=config.IMAGE_MAX_DIM)\n",
    "            print(image.shape)\n",
    "            print('Image resized at: ', molded_image.shape)\n",
    "            print(window)\n",
    "            print(scale)\n",
    "            \"\"\"Takes RGB images with 0-255 values and subtraces\n",
    "                   the mean pixel and converts it to float. Expects image\n",
    "                   colors in RGB order.\"\"\"\n",
    "            molded_image = mold_image(molded_image, config)\n",
    "            print('Image molded')\n",
    "            #print(a)\n",
    "            \"\"\"Takes attributes of an image and puts them in one 1D array.\"\"\"\n",
    "            image_meta = compose_image_meta(\n",
    "                0, image.shape, window,\n",
    "                np.zeros([config.NUM_CLASSES], dtype=np.int32))\n",
    "            print('Meta of image prepared')\n",
    "            # Append\n",
    "            molded_images.append(molded_image)\n",
    "            windows.append(window)\n",
    "            image_metas.append(image_meta)\n",
    "        # Pack into arrays\n",
    "        molded_images = np.stack(molded_images)\n",
    "        image_metas = np.stack(image_metas)\n",
    "        windows = np.stack(windows)\n",
    "        return molded_images, image_metas, windows\n",
    "\n",
    "def mold_image(images, config):\n",
    "    return images.astype(np.float32) - config.MEAN_PIXEL\n",
    "\n",
    "def compose_image_meta(image_id, image_shape, window, active_class_ids):\n",
    "    \"\"\"Takes attributes of an image and puts them in one 1D array.\n",
    "\n",
    "    image_id: An int ID of the image. Useful for debugging.\n",
    "    image_shape: [height, width, channels]\n",
    "    window: (y1, x1, y2, x2) in pixels. The area of the image where the real\n",
    "            image is (excluding the padding)\n",
    "    active_class_ids: List of class_ids available in the dataset from which\n",
    "        the image came. Useful if training on images from multiple datasets\n",
    "        where not all classes are present in all datasets.\n",
    "    \"\"\"\n",
    "    meta = np.array(\n",
    "        [image_id] +            # size=1\n",
    "        list(image_shape) +     # size=3\n",
    "        list(window) +          # size=4 (y1, x1, y2, x2) in image cooredinates\n",
    "        list(active_class_ids)  # size=num_classes\n",
    "    )\n",
    "    return meta\n",
    "\n",
    "def unmold_detections(detections, mrcnn_mask, image_shape, window):\n",
    "    \"\"\"Reformats the detections of one image from the format of the neural\n",
    "    network output to a format suitable for use in the rest of the\n",
    "    application.\n",
    "\n",
    "    detections: [N, (y1, x1, y2, x2, class_id, score)]\n",
    "    mrcnn_mask: [N, height, width, num_classes]\n",
    "    image_shape: [height, width, depth] Original size of the image before resizing\n",
    "    window: [y1, x1, y2, x2] Box in the image where the real image is excluding the padding.\n",
    "\n",
    "        Returns:\n",
    "        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n",
    "        class_ids: [N] Integer class IDs for each bounding box\n",
    "        scores: [N] Float probability scores of the class_id\n",
    "        masks: [height, width, num_instances] Instance masks\n",
    "        \"\"\"\n",
    "    # How many detections do we have?\n",
    "    # Detections array is padded with zeros. Find the first class_id == 0.\n",
    "    zero_ix = np.where(detections[:, 4] == 0)[0]\n",
    "    N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n",
    "    print('Number of detections: ',N)\n",
    "    print('Window: ',window)\n",
    "    # Extract boxes, class_ids, scores, and class-specific masks\n",
    "    boxes = detections[:N, :4]\n",
    "    print('boxes',boxes.shape,' ',boxes)\n",
    "    class_ids = detections[:N, 4].astype(np.int32)\n",
    "    print('Class_ids: ',class_ids.shape,' ',class_ids)\n",
    "    scores = detections[:N, 5]\n",
    "    print('Scores: ',scores.shape,' ',scores)\n",
    "    masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n",
    "    print('Masks: ',masks.shape)# masks)\n",
    "    # Compute scale and shift to translate coordinates to image domain.\n",
    "    print(image_shape[0])\n",
    "    print(window[2] - window[0])\n",
    "    h_scale = image_shape[0] / (window[2] - window[0])\n",
    "    print('h_scale: ',h_scale)\n",
    "    w_scale = image_shape[1] / (window[3] - window[1])\n",
    "    print('w_scale: ',w_scale)\n",
    "    scale = min(h_scale, w_scale)\n",
    "    shift = window[:2]  # y, x\n",
    "    print('shift: ',shift)\n",
    "    scales = np.array([scale, scale, scale, scale])\n",
    "    print('scales: ',scales)\n",
    "    shifts = np.array([shift[0], shift[1], shift[0], shift[1]])\n",
    "    print('shifts: ',shifts)\n",
    "    # Translate bounding boxes to image domain\n",
    "    boxes = np.multiply(boxes - shifts, scales).astype(np.int32)\n",
    "    print('boxes: ',boxes.shape,' ',boxes)\n",
    "    # Filter out detections with zero area. Often only happens in early\n",
    "    # stages of training when the network weights are still a bit random.\n",
    "    exclude_ix = np.where(\n",
    "        (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n",
    "    if exclude_ix.shape[0] > 0:\n",
    "        boxes = np.delete(boxes, exclude_ix, axis=0)\n",
    "        class_ids = np.delete(class_ids, exclude_ix, axis=0)\n",
    "        scores = np.delete(scores, exclude_ix, axis=0)\n",
    "        masks = np.delete(masks, exclude_ix, axis=0)\n",
    "        N = class_ids.shape[0]\n",
    "\n",
    "    # Resize masks to original image size and set boundary threshold.\n",
    "    full_masks = []\n",
    "    for i in range(N):\n",
    "        # Convert neural network mask to full size mask\n",
    "        full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape)\n",
    "        full_masks.append(full_mask)\n",
    "    full_masks = np.stack(full_masks, axis=-1)\\\n",
    "        if full_masks else np.empty((0,) + masks.shape[1:3])\n",
    "\n",
    "    return boxes, class_ids, scores, full_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create correctly shaped tuple from [(0, 0), (255, 255), (0, 0)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bc28ad7beea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmolded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmold_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Images meta: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_metas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-44653e501f10>\u001b[0m in \u001b[0;36mmold_inputs\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     19\u001b[0m             molded_image, window, scale, padding = utils.resize_image(\n\u001b[1;32m     20\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 max_dim=config.IMAGE_MAX_DIM)\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image resized at: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmolded_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gustav/workspace/Mobile_Mask_RCNN/mmrcnn/utils.pyc\u001b[0m in \u001b[0;36mresize_image\u001b[0;34m(image, min_dim, max_dim, min_scale, mode)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mright_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_dim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleft_pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtop_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtop_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pad64\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0mnarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     allowedkwargs = {\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36m_validate_lengths\u001b[0;34m(narray, number_elements)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \"\"\"\n\u001b[0;32m-> 1080\u001b[0;31m     \u001b[0mnormshp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormshp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mchk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36m_normalize_shape\u001b[0;34m(ndarray, shape, cast_to_int)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Unable to create correctly shaped tuple from %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;31m# Cast if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create correctly shaped tuple from [(0, 0), (255, 255), (0, 0)]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mmrcnn import utils\n",
    "from mmrcnn import visualize\n",
    "\n",
    "with tf.gfile.FastGFile(PB_MODEL_PATH, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "print('Graph loaded.')\n",
    "\n",
    "testImage =os.path.join(ROOT_DIR,'ski.jpg') #image of the size defined in the config\n",
    "image = cv2.cvtColor(cv2.imread(testImage), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "molded_images, image_metas, windows = mold_inputs(image)\n",
    "print(molded_images.shape)\n",
    "print('Images meta: ',image_metas)\n",
    "img_ph = sess.graph.get_tensor_by_name('input_image:0')\n",
    "print(img_ph)\n",
    "img_meta_ph = sess.graph.get_tensor_by_name('input_image_meta:0')\n",
    "print(img_meta_ph)\n",
    "detectionsT = sess.graph.get_tensor_by_name('output_detections:0')\n",
    "print('Found ',detectionsT)\n",
    "mrcnn_classT = sess.graph.get_tensor_by_name('output_mrcnn_class:0')\n",
    "print('Found ',mrcnn_classT)\n",
    "mrcnn_bboxT = sess.graph.get_tensor_by_name('output_mrcnn_bbox:0')\n",
    "print('Found ', mrcnn_bboxT)\n",
    "mrcnn_maskT = sess.graph.get_tensor_by_name('output_mrcnn_mask:0')\n",
    "print('Found ', mrcnn_maskT)\n",
    "roisT = sess.graph.get_tensor_by_name('output_rois:0')\n",
    "print('Found ', roisT)\n",
    "        \n",
    "detections = sess.run(detectionsT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Detections: ',detections[0].shape, detections[0])\n",
    "mrcnn_class = sess.run(mrcnn_classT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Classes: ',mrcnn_class[0].shape, mrcnn_class[0])\n",
    "mrcnn_bbox = sess.run(mrcnn_bboxT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('BBoxes: ',mrcnn_bbox[0].shape, mrcnn_bbox[0])\n",
    "mrcnn_mask = sess.run(mrcnn_maskT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Masks: ',mrcnn_mask[0].shape )#, outputs1[0])\n",
    "rois = sess.run(roisT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Rois: ',rois[0].shape, rois[0])\n",
    "\n",
    "results = []\n",
    "for i, image in enumerate(images):\n",
    "    print('Calculating results for image#',i)\n",
    "    final_rois, final_class_ids, final_scores, final_masks =\\\n",
    "    unmold_detections(detections[i], mrcnn_mask[i],\n",
    "                                    image.shape, windows[i])\n",
    "    results.append({\n",
    "        \"rois\": final_rois,\n",
    "        \"class_ids\": final_class_ids,\n",
    "        \"scores\": final_scores,\n",
    "        \"masks\": final_masks,\n",
    "    })\n",
    "r = results[0]\n",
    "#print(r)\n",
    "print (r['scores'][0])\n",
    "print (r['class_ids'][0])\n",
    "print (r['rois'][0])\n",
    "print (r['masks'][0].shape)\n",
    "\n",
    "class_names = [\"BG\",\"Person\"]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'], ax=get_ax())\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
